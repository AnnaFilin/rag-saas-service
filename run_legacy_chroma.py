
# run.py
# --------------------------------------------
# Entry point for the Memory Archive RAG system.
# Modes:
#   - USE_UI = True  ‚Üí launch Gradio interface
#   - USE_UI = False ‚Üí run console mode (prints answers via enhanced_query_with_llm)

from pathlib import Path
import chromadb

from src.load_docs import load_local_documents
from src.process_texts import split_into_chunks, analyze_chunks
from src.build_index import create_embeddings, store_in_chroma
from src.llm_pipeline import build_llm_chain, enhanced_query_with_llm
from src.stream_interface import launch_demo

# ======= CONFIG =======
USE_UI = True                      # Toggle between UI and console modes
COLLECTION_NAME = "python_guide"   # Name of Chroma collection
INDEX_PATH = Path("./data/index/chroma_db")  # Persistent index path
DOC_FILES = [
    "documents/9241545178.pdf",
    "documents/TheEncyclopediaOfPsychoactivePlants.pdf",
]
MAX_CHUNKS = 2000                  # Soft limit to prevent heavy first-run embedding
EMBED_MODEL_NAME = "multi-qa-mpnet-base-dot-v1"
# =======================


def _load_existing_index():
    """Load an existing Chroma index and SentenceTransformer model."""
    print("‚ö° Using existing Chroma index ‚Äî skipping regeneration.")
    from sentence_transformers import SentenceTransformer

    model = SentenceTransformer(EMBED_MODEL_NAME)
    client = chromadb.PersistentClient(path=str(INDEX_PATH))
    collection = client.get_or_create_collection(name=COLLECTION_NAME)
    return model, collection


def _build_index_from_documents():
    """
    Full index creation pipeline:
    1. Load local documents
    2. Split them into chunks
    3. Limit total chunks (optional)
    4. Create embeddings
    5. Store in Chroma
    """
    # 1) Load local PDFs
    all_docs = load_local_documents(DOC_FILES)

    # 2) Split into semantic chunks
    all_chunks = []
    for doc in all_docs:
        chunks = split_into_chunks(doc["content"], doc["source"])
        all_chunks.extend(chunks)
    analyze_chunks(all_chunks)

    # 3) Apply a safety limit for the initial run
    if len(all_chunks) > MAX_CHUNKS:
        print(f"‚ö†Ô∏è Limiting processing to first {MAX_CHUNKS} chunks out of {len(all_chunks)} total for safety.")
        all_chunks = all_chunks[:MAX_CHUNKS]

    # 4) Generate embeddings and store them in Chroma
    embeddings, model = create_embeddings(all_chunks)
    collection = store_in_chroma(
        all_chunks,
        embeddings,
        db_path=str(INDEX_PATH),
        collection_name=COLLECTION_NAME
    )
    return model, collection


def main():
    """Main entry point ‚Äî loads or builds index, initializes LLM, and runs UI or console mode."""
    # 0) Prepare model and collection
    if INDEX_PATH.exists():
        model, collection = _load_existing_index()
    else:
        # Build index once, then reuse it for future runs
        model, collection = _build_index_from_documents()

    # 1) Initialize the LLM chain
    chain = build_llm_chain()
    print("ü¶ô LLM initialized.")

    if USE_UI:
        # 2A) Launch Gradio-based UI mode
        print("üåê Launching Gradio interface‚Ä¶")
        launch_demo(model, collection, chain)
    else:
        # 2B) Console mode ‚Äî directly query the RAG system
        question = "List specific plants mentioned in these books that are said to induce visionary or dream states. Include short notes about each."
        response = enhanced_query_with_llm(question, model, collection, chain)
        print("\n" + response)


if __name__ == "__main__":
    main()
