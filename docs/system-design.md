# RAG SaaS – System Design

## 1. Goal

A small multi-workspace RAG (Retrieval-Augmented Generation) service:

- Users can create **workspaces** and upload documents into them.
- The system builds a **vector index** over these documents.
- Users can ask questions with an optional **"agent role"** description.
- Answers are generated by an external **LLM** using retrieved context.
- The service runs in the cloud, can be shared with a few friends, and is cost-controlled.

---

## 2. Architecture Overview

- **Frontend**
  - Simple SPA (React or Vue) with:
    - Workspace selection.
    - Document upload screen.
    - Chat screen (question + role + answer history).

- **Backend**
  - Python + FastAPI (or similar).
  - REST API endpoints for:
    - Health check (`/health`).
    - Ingestion (`/ingest`).
    - Chat (`/chat`).
  - Deployed to a serverless environment (e.g. Cloud Run).

- **Vector Database**
  - Option A: Postgres + `pgvector` (e.g. Neon).
  - Option B: Managed vector DB (e.g. Qdrant Cloud).
  - Used to store chunk embeddings and perform similarity search.
  - **Vector Database**
  - Neon Postgres + pgvector (primary choice)


- **LLM Provider**
  - Primary: external API (e.g. OpenAI, cheap model family).
  - Future: optional self-hosted open-source model (e.g. LLaMA) behind the same interface.
- **LLM Provider**
  - Primary: OpenAI (cheap model family, e.g. gpt-4o-mini + text-embedding-3-small).
  - Future: optional self-hosted open-source model (e.g. LLaMA) behind the same interface.

---

## 3. Data Model and Limits (Cost Protection)

### 3.1 Workspaces

Each workspace represents a logical collection of documents.

Fields (conceptual):

- `id`: string (internal identifier).
- `name`: string.
- `description`: string (optional).
- `created_at`: timestamp.

Limits (to be refined):

- Max workspaces per user/IP: **TBD** (e.g. 3–5).

### 3.2 Documents

Documents belong to a workspace and are the source of text for indexing.

Fields:

- `id`: string.
- `workspace_id`: string.
- `title`: string.
- `source_type`: enum (`file`, `url`, `raw_text`).
- `original_path_or_url`: string.
- `created_at`: timestamp.

Limits:

- Supported types: `pdf`, `txt`, `md` (initially).
- Max file size: **TBD** (e.g. 2–5 MB).
- Max documents per workspace: **TBD** (e.g. 50).
- Optional: max total text size per workspace (approximate token budget).

### 3.3 Chunks

Chunks are the atomic units for retrieval and embedding.

Fields:

- `id`: string.
- `workspace_id`: string.
- `document_id`: string.
- `chunk_index`: integer.
- `text`: string.
- `embedding`: vector (pgvector / vector field).
- `metadata`: JSON (optional; e.g. page number).

Chunking strategy:

- Chunk size: **TBD** (e.g. 300–500 tokens).
- Overlap: **TBD** (e.g. 50–100 tokens).
- Max chunks per workspace: **TBD** (e.g. 3k–5k chunks).

---

## 4. Retrieval & LLM Configuration

### 4.1 Embeddings

- Provider: **TBD** (e.g. OpenAI `text-embedding-3-small` or similar).
- Embedding dimension: depends on chosen model.
- Embedding usage:
  - Create vector for each chunk.
  - Create vector for each user question.

Cost-related limits:

- Max total embedding calls per month: **TBD** (target budget, e.g. ≤ $5).
- Avoid re-embedding the same text (cache by hash).

### 4.2 Vector Search

- Similarity metric: cosine similarity / inner product (depends on DB).
- Base query:
  - Filter by `workspace_id`.
  - Order by vector distance.
  - Limit: `top_k` chunks (e.g. 3–5).

Parameters to fix:

- `top_k`: **TBD** (e.g. 3–4).
- Optional metadata filters (e.g. by document or type) later.

### 4.3 LLM Generation

- Provider: external API (e.g. `gpt-4o-mini`-class or similar).
- Input:
  - User question.
  - Agent role description.
  - Retrieved chunks (as context, trimmed to max context length).

LLM parameters (initial):

- `max_tokens`: **TBD** (e.g. 300–500).
- `temperature`: **TBD** (e.g. 0.2–0.4).
- `top_p`: default or **TBD**.
- System prompt: instruct to:
  - Use only provided context.
  - Admit unknown if context is not enough.

Context control:

- Max total context length (chunks + question + role): **TBD** (e.g. 1500–2000 tokens).
- Hard trimming of context to avoid token explosion.

---

## 5. User-Facing Limits and Rate Control

### 5.1 Question Input

- Max question length: **TBD** (e.g. 500–1000 characters).
- Max role description length: **TBD** (e.g. 300–500 characters).

### 5.2 Per-IP / Per-User Limits

- Max chat requests per IP per day: **TBD** (e.g. 50).
- Max ingestion operations per IP per day: **TBD**.

Behavior on limit reached:

- Return clear error:  
  `"Daily limit reached for this IP. Please try again tomorrow."`

### 5.3 Token/Budget Awareness

- Log approximate token usage per request (prompt + completion).
- Optional: store rolling statistics in DB.
- Target monthly LLM budget: **TBD** (e.g. ≤ $5).

---

## 6. LLM Abstraction and Kill Switch

### 6.1 LLMClient Interface

Backend will use an internal abstraction instead of directly calling a specific provider.

Pseudo-interface:

- `embed(texts: list[str]) -> list[vector]`
- `generate_answer(question: str, context_chunks: list[str], role: str) -> str`

Implementations:

- `OpenAILLMClient` (initial, default).
- Future: `SelfHostedLLMClient` (e.g. LLaMA on GPU VM).

### 6.2 Kill Switch

- Env var: `LLM_ENABLED=true/false`.
- If `LLM_ENABLED=false`:
  - `/chat` returns a controlled error:
    - `"LLM is temporarily disabled. Please try again later."`

Use cases:

- Emergency shutdown if API usage spikes.
- Manual pause while debugging or redeploying.


### 6.3 LLM Backend Selection

The system must support more than one LLM backend under the same `LLMClient` interface:

- `OpenAILLMClient` – external provider (default).
- `LocalLLMClient` – self-hosted model (e.g. LLaMA on a GPU VM or Ollama).

#### Configuration

- Env var: `ALLOWED_LLM_BACKENDS` (comma-separated), e.g. `external,local`.
- Env var: `DEFAULT_LLM_BACKEND` (e.g. `external`).

#### Workspace-level setting

Each workspace can optionally store its preferred LLM backend:

- Field `llm_backend` with allowed values:
  - `"external"`
  - `"local"`
- If not set, `DEFAULT_LLM_BACKEND` is used.

#### Request-level override

The `/chat` endpoint may accept an optional `backend` field:

```json
{
  "workspace_id": "example-ws",
  "role": "Agent role description",
  "question": "User question",
  "backend": "external" // or "local"
}

---

## 7. Implementation Plan (Phases)

### Phase 1 – Vector DB + Backend Skeleton

- Choose and set up vector DB (Postgres + pgvector or Qdrant Cloud).
- Define DB schema: workspaces, documents, chunks.
- Implement ingestion pipeline (chunking + embeddings + index write).
- Implement endpoints:
  - `GET /health`
  - `POST /ingest` (no LLM usage yet).

### Phase 2 – Retrieval + LLM Integration

- Implement vector search (top_k by similarity).
- Implement `/chat` endpoint:
  - Embed question.
  - Retrieve top_k chunks for workspace.
  - Call `LLMClient.generate_answer`.
  - Apply limits from sections 3–5.
- Add LLM kill switch and basic logging of token usage.

### Phase 3 – Frontend (Minimal UI)

- Build SPA with:
  - Workspace selector.
  - Upload screen (with basic progress/error messages).
  - Chat screen (question + role + answer history).
- Connect SPA to backend endpoints.

### Phase 4 – Rate Limiting and Polishing

- Implement per-IP limits in backend.
- Better error handling and UX messages.
- Basic styling and documentation updates.

---

## 8. Open Questions / TODO

- Choose final vector DB provider: **Neon + pgvector vs Qdrant Cloud**.
- Choose initial LLM and embedding models (provider + exact names).
- Decide concrete numeric limits for:
  - Max file size.
  - Max documents per workspace.
  - Max chunks / workspace.
  - Max requests per IP per day.
  - Target monthly budget.

## API – /chat (initial contract)

### Endpoint

- `POST /chat`

### Request body (JSON)

```json
{
  "workspace_id": "string",
  "question": "string",
  "role": "optional string (can be null)"
}


## API – /ingest (initial contract)

### Endpoint

- `POST /ingest`

### Request body (JSON, упрощённый вариант для начала)

```json
{
  "workspace_id": "string",
  "documents": [
    {
      "id": "optional string",
      "text": "string",
      "metadata": {
        "source": "optional string",
        "title": "optional string"
      }
    }
  ]
}
